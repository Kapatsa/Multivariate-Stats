\documentclass[12pt,a4paper,final]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[OT1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows,matrix,positioning}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{bm}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\newtheorem{proposition}{Предложение}
\newtheorem{theorem}{Теорема}

\author{Дейвид Капаца}
\title{Краткие ответы: коллоквиум по PCA}
\begin{document}
\maketitle
\tableofcontents

\section{Общая информация}

\subsection{Введённые обозначения}

\begin{itemize}
\item $a,\, b,\, c,\ldots$ --- скалярные величины;
\item $\xi, \eta, \ldots$ --- случайные величины;
\item $\bm \xi, \bm \eta, \ldots$ --- случайные векторы;
\item $\bm X,\, \bm Y, \ldots$ --- матрицы;
\item $X_i$ --- вектор-столбцы матрицы $\bm X$;
\item $\bm x_i$ --- вектор-строки матрицы $\bm X$;

\end{itemize}

\subsection{Замечания про умножение матриц}

Умножение матриц бывает полезным понимать как некоторые операции над столбцами и строками перемножаемых матриц $\bm A$ и $\bm B$. Рассмотрим две такие интерпретации.

\paragraph*{Скалярное произведение}
Пожалуй, самая простая из интерпретаций --- через скалярное произведение столбцов $\bm A$ на строки $\bm B$. Рассмотрим наглядный пример.

\begin{equation*}
\bm A \bm B =
    \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            a_{11} & a_{12}  \\               
            a_{21} & a_{22}  \\                       
        };  
        \draw[color=orange] (m-1-1.north west) -- (m-1-2.north east) -- (m-1-2.south east) -- (m-1-1.south west) -- (m-1-1.north west);
    \end{tikzpicture}
        \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            b_{11} & b_{12}  \\               
            b_{21} & b_{22}  \\                       
        };  
        \draw[color=orange] (m-1-1.north west) -- (m-1-1.north east) -- (m-2-1.south east) -- (m-2-1.south west) -- (m-1-1.north west);
    \end{tikzpicture}
    =         \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
           \langle \bm a_1, \bm B_1 \rangle & \langle \bm a_1, \bm B_2 \rangle \\               
           \langle \bm a_2, \bm B_1 \rangle & \langle \bm a_2, \bm B_2 \rangle  \\                       
        };  
        \draw[color=orange] (m-1-1.north west) -- (m-1-1.north east) -- (m-1-1.south east) -- (m-1-1.south west) -- (m-1-1.north west);
    \end{tikzpicture}
\end{equation*}
Очевидно, что если хотим рассмотреть скалярные произведения столбцов $A$ на столбцы $B$, можно использовать транспонирование: $\bm A^\mathrm{T} \bm B$. Аналогично для склярного произведения строк: $\bm A \bm B^\mathrm{T}$. Если вспомнить, что скалярные произведения имеют наглядную геометрическую интерпретацию,  данный аппарат может быть весьма полезен. 

\paragraph*{Линейная комбинация}

Ещё одна очень полезная интерпретация заключается в том, что строки результата произведения матриц есть линейная комбинация столбцов исходной.
Покажем это в двумерном случае: 

\begin{equation*}
\bm A \bm B =
    \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            a_{11} & a_{12}  \\               
            a_{21} & a_{22}  \\                       
        };  
        \draw[color=orange] (m-1-1.north west) -- (m-1-1.north east) -- (m-2-1.south east) -- (m-2-1.south west) -- (m-1-1.north west);
                \draw[color=orange] (m-1-2.north west) -- (m-1-2.north east) -- (m-2-2.south east) -- (m-2-2.south west) -- (m-1-2.north west);
    \end{tikzpicture}
        \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
            b_{11} & b_{12}  \\               
            b_{21} & b_{22}  \\                       
        };  
        \draw[color=orange] (m-1-1.north west) -- (m-1-1.north east) -- (m-1-1.south east) -- (m-1-1.south west) -- (m-1-1.north west);
         \draw[color=orange] (m-2-1.north west) -- (m-2-1.north east) -- (m-2-1.south east) -- (m-2-1.south west) -- (m-2-1.north west);
    \end{tikzpicture}
    =         \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
          a_{11} b_{11} + a_{12} b_{21}   &   a_{11} b_{12} + a_{12} b_{22} \\
          a_{21} b_{11} + a_{22} b_{21}   &   a_{21} b_{12} + a_{22} b_{22} \\
        };  
        \draw[color=orange] (m-1-1.north west) -- (m-1-1.north east) -- (m-2-1.south east) -- (m-2-1.south west) -- (m-1-1.north west);
    \end{tikzpicture}
    =
\end{equation*}

\begin{equation*}
=
         \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \matrix [matrix of math nodes,left delimiter=(,right delimiter=)] (m)
        {
         b_{11} A_1 + b_{21} A_2  &   b_{12} A_1 + b_{21} A_2 \\
        };  
        \draw[color=orange] (m-1-1.north west) -- (m-1-1.north east) -- (m-1-1.south east) -- (m-1-1.south west) -- (m-1-1.north west);
    \end{tikzpicture}
\end{equation*}

\noindent Таким образом, если столбцы матрицы $\bm A$ представляют из себя некоторый базис, то элементы столбцов матрицы $\bm B$ есть координаты полученного вектор-столбца в этом базисе.

\subsubsection*{Новый признак как линейная комбинация старых}

Пусть мы хотим получить новый признак $\eta$ как линейную комбинацию старых признаков $\xi_1,\,\xi_2,\ldots , \xi_n$. Так и запишем:
$$
\eta = a_1\xi_1 + a_2\xi_2 + \ldots + a_n\xi_n = \bm a^\textrm{T} \bm \xi.
$$

\noindent При этом, если ковариационная матрица для $\bm \xi$ есть $\bm \Sigma$, то $$ \mathrm D \eta = \bm a^\mathrm{T} \bm\Sigma \bm a.$$

\noindent Теперь с помощью линейных преобразований над элементами вектора $\bm \xi$ получим $d$-мерную случайную величину $\bm \eta = \left(\eta_1, \eta_2, \ldots, \eta_d\right)^\mathrm{T}$:
$$
\bm \eta = \left(\begin{matrix}
\eta_1 = \bm a_1^\mathrm{T} \bm\xi \\ \eta_2 = \bm a_2^\mathrm{T} \bm\xi \\ \vdots \\  \eta_d = \bm a_d^\mathrm{T} \bm\xi
\end{matrix} \right) = \bm A^\mathrm{T}\bm\xi.
$$

\noindent Если $\bm A = \left( \bm a_1 | \bm a_2 | \cdots | \bm a_d \right) \in \mathbb R^{n \times d}$, то ковариационная матрица для $\bm\eta$ имеет вид: $$\textrm{Cov}(\bm\eta) = \bm A^\mathrm{T} \bm \Sigma \bm A.$$

\subsection{Основные свойства ортогональных матриц}

\textbf{Ортогональной матрицей} называется такая матрица $\bm A$, что $\bm A^\mathrm{T}\bm A = \bm A \bm A ^\mathrm{T} = \bm E$, где $\bm E$ --- единичная матрица.

\noindent Из определения очевидно, что ортогональность матрицы $\mathbb A$ \textit{равносильна} тождеству $$\bm A^\mathrm{T} = \bm A^{-1}.$$

\noindent Подробнее рассмотрим произведение $\bm A^\mathrm{T} \bm A$:

\begin{itemize}
\item С одной стороны, $\bm A \bm A^\mathrm{T}$ --- это матрица скалярных произведений строчек матрицы $\bm A$ на строчки $\bm A$;
\item С другой стороны, так как $\bm A$ --- ортогональна, так что по определению  $\bm A \bm A^\mathrm{T} = \bm E$. 
\end{itemize}
Отсюда сразу следует, что строчки исходной матрицы $\bm A$ ортогональны между собой. Более того, так как на диагонали полученной матрицы $\bm A \bm A^\mathrm{T}$ единицы, то норма строчек $\|A_{i}\| = \sqrt{\langle A_{i}, A_{i} \rangle} = 1$, то есть они ещё и нормированы. Аналогично для столбцов.

Важное свойство ортогональных матриц: умножение на ортогональную матрицу означает поворот или отражение. При этом не меняются нормы векторов и углы между ними.

\section{Краткие ответы}

\subsection{Многомерное нормальное распределение. Вектор мат. ож. и ковар. матрица при линейном преобразовании (умножении на матрицу)}

Пусть с.в. $\bm \xi \sim \textrm{N}(\bm a, \bm \Sigma)$, где $\bm a \in \mathbb R^n$ --- вектор средних, $\bm \Sigma \in \mathbb R^{n \times n}$ --- ковар. матрица.
Плотность многомерного нормального распределения:
$$
p(\bm x) = \frac{|\bm \Sigma|^{-1/2}}{(2\pi)^{1/n}}\, \textrm{exp}\left(-\frac{1}{2}(\bm x - \bm a)^\mathrm{T} \bm \Sigma^{-1}(\bm x - \bm a)\right).
$$

Пусть $\bm A$ -- некоторая матрица $m \times n$. Пусть есть с.в. $\bm \xi \in \mathbb R^{n}$ с м.о. $\bm a$ и ковариац. матрицей $\bm \Sigma$. Как будут выглядеть м.о. и ковар. матрица с.в. $\bm A\bm \xi$? С м.о. всё понятно:
$$
\mathrm{E}(\bm A \bm \xi) = \bm A \mathrm{E}(\bm \xi) = \bm A \bm a,
$$

Теперь ковариация:
\begin{multline*}
\mathrm{Cov}(\bm A \bm \xi) = \mathrm{E}(\bm A \bm \xi - \mathrm{E} \bm A \bm \xi)(\bm A \bm \xi - \mathrm{E} \bm A \bm \xi)^\mathrm{T} = \mathrm{E}(\bm A \bm \xi - \bm A \bm a)(\bm A \bm \xi - \bm A \bm a)^\mathrm{T} = \\ = \mathrm{E}(\bm A (\bm \xi - \bm a))(\bm A (\bm \xi - \bm a))^\mathrm{T} = \mathrm{E}(\bm A (\bm \xi - \bm a))((\bm \xi - \bm a)^\mathrm{T}\bm A^\mathrm{T}) = \bm A \mathrm{E}( (\bm \xi - \bm a))((\bm \xi - \bm a)^\mathrm{T}\bm)  \bm A^\mathrm{T} = \bm A \mathrm{Cov}(\bm \xi) \bm A^\mathrm{T}
\end{multline*}

\subsection{Оценки вектора мат. ожиданий и ковар. матрицы. Несмещённая оценка ковар. матрицы}

Здесь всё просто. Пусть векторы $\bm x_i, \, i \in 1:n$ представляют из себя выборку некоторого многомерного случайного вектора $\bm \xi$. С вектором средних всё понятно, обозначим его $\bar{\bm x}$. Оценка ковариационной матрицы строится по определению.

$$
\widehat{\mathrm{Cov(\bm \xi)}} = \frac{1}{n} \sum_{i = 1}^{n} (\bm x_i - \bar{\bm x})(\bm x_i - \bar{\bm x})^\mathrm{T}
$$

С несмещённой оценкой вот так:

$$
\widehat{\mathrm{Cov(\bm \xi)}}_\textit{unbiased} = \frac{1}{n-1} \sum_{i = 1}^{n} (\bm x_i - \bar{\bm x})(\bm x_i - \bar{\bm x})^\mathrm{T}
$$


\subsection{Распределение вектора выборочных средних}

Если случайный вектор имеет конечный второй момент, то верно следующее:

$$
\bm{\bar x} \xrightarrow{\sim} \mathrm N\left(\bm a,\frac{ \bm \Sigma}{n}\right)
$$
В частном случае нормального распределения значок асимптотики $\sim$ можем убрать.

\subsection{Переход к новым признакам с помощью ортогональной матрицы. Пример про способности по математике и физике (выписать матрицу вращения)}

С помощью ортогональной матрицы можно перейти к новым признакам. Возьмём матрицу $\bm X = \left( X_1 |  X_2 | \ldots |  X_p \right) $ и рассмотрим произведение 
$
\bm A \bm X,
$
где $\bm A \in \mathbb R^{n \times n}$ --- ортогональная матрица. Хорошо известно, что применение ортогонального оператора к матрице задаёт некоторый поворот или отражение вектор-столбцов матрицы $\bm X$. При этом каждый столбец результирующей матрицы $\bm A \bm X$ будет линейной комбинацией столбцов матрицы $A$.

\subsubsection*{Пример про способности по математике и физике}

Пусть признаки такие:
\begin{itemize}
\item $X_1$ --- оценки по математике;
\item $X_2$ --- оценки по физике;
\item $X_3$ --- оценки по русскому;
\item $X_4$ --- оценки по литературе.
\end{itemize}

\noindent Итак, $\bm X = \left( X_1 |  X_2 | X_3 |  X_4 \right).$ Домножением матрицы $\bm X$ на $\bm A$ получим матрицу с двумя новыми признаками $Z_1$ и $Z_2$, которые отвечают за общую оценку по техническим и гуманитарным наукам.

$$
\bm X \bm A = \underbrace{\left( X_1 |  X_2 | X_3 |  X_4 \right)}_{\bm X} \underbrace{\left( \begin{matrix} 1 & 0 \\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \end{matrix} \right)}_{\bm A} =  \left( X_1 + X_2 \quad X_3 + X_4 \right).
$$

\subsection{Сингулярное разложение, как строится}

\textbf{Сингулярным разложением} матрицы $\bm Y := \bm X^\mathrm{T}$  называется её разложение в сумму следующего вида:
$$
\bm Y = \sum_{i = 1}^d \sqrt{\lambda_i} U_i V_i^\mathrm{T},
$$
где $\lambda_i$ --- собственные числа матрицы $\bm Y \bm Y^\mathrm{T}$, $U_i$ --- её собственные векторы, $V_i$ --- собственные векторы матрицы $\bm Y^\mathrm{T} \bm Y$, $d$ --- её ранг.

\dotfill

\noindent Пусть у нас $L$ признаков и $K$ индивидов.
Пусть также $\bm Y := \bm X^\mathrm{T}$.
Возьмём матрицу $$\bm S = \bm Y \bm Y^\mathrm{T} \in \mathbb{R}^{L \times L}.$$ Пусть $\lambda_i$ --- собственные числа матрицы, $U_i$ --- её собственные векторы, что по определению означает
$$
\bm S U_i = \lambda_i U_i.
$$
Собственных чисел у нас $L$ штук; теперь, если упорядочить их по убыванию, то число \textit{ненулевых} собственных чисел будет равно рангу матрицы $\bm S$. Число $d$ --- ранг матриц $\bm Y$ и $\bm S$,\footnote{ядра этих операторов совпадают, поэтому можно показать, что их ранги равны} он же ранг пространства столбцов и строк $\bm Y$, будет не больше, чем $\min{(L,\,K)}$. 

Важно, что $\{U_i\}_{i = 1}^{d}$ образуют \textit{ортонормированный базис пространства столбцов} матрицы $\bm Y$.\footnote{если вернуться к матрице $\bm  X$, то это означает \textit{ортонормированный базис пространства строк-индивидов}}
Введём также векторы $V_i$, которые, как утверждается, сформируют ортонормированный базис пространства строк матрицы $\bm Y$ и будут собственными векторами матрицы $\bm Y^\mathrm{T} \bm Y$:
$$
V_i = \frac{\bm Y^\mathrm{T} U_i}{\sqrt \lambda_i}.
$$ 
Можно проверить, что тогда
$$
U_i = \frac{\bm Y V_i}{\sqrt{\lambda_i}}.
$$
Формула сингулярного разложения имеет вид:
$$
\bm Y = \sum_{i = 1}^d \sqrt{\lambda_i} U_i V_i^\mathrm{T},
$$
$V_i$ --- это новые признаки. В матричном виде:
$$
\bm Y = \bm U \bm \Lambda^{1/2} \bm V^\mathrm{T}.
$$

\subsection{Сингулярное разложение, в каком смысле оно единственно}

Если у нас есть некоторое сингулярное разложение с векторами $U_i$ и $V_i$, ничего не поменяется, если возьмём в качестве множителей векторы $-U_i$ и $-V_i$.

Также может возникнуть неединственность, когда у нас есть два или более равных собственных числа. Тогда любая нетривиальная линейная комбинация векторов, соответствующих этому собственному числу, будет давать собственный вектор, соответствующий тому же собственному числу. Другими словами, можно выбрать любой ортонормированный базис соответствующего подпространства в качестве новых собственных векторов.

Поэтому можно сформулировать следующее утверждение о единственности сингулярного разложения:

\begin{proposition}[Единственность SVD]
Пусть $L\leqslant K$. Если $\bm Y = \sum_{i = 1}^{L} c_i P_i Q_i^\mathrm{T}$ есть некоторое биортогональное разложение матрицы, то есть:
\begin{enumerate}
\item $c_1 \geqslant c_2 \geqslant \ldots \geqslant c_L \geqslant 0$;
\item $\{P_i\}$ и $\{Q_i\}$ --- ортонормированные.
\end{enumerate}
Любое биортогональное разложение есть SVD.
\end{proposition}

\subsection{Выборочный анализ главных компонент и сингулярное разложение, общее и различия}

\textbf{(?)} Когда сравниваем разложение из SVD и PCA, у нас $\bm Y$ в обоих случаях уже с центрированными столбцами? (иначе ведь они отличаются друг от друга; в конспекте написано не совсем понятно когда центрируем столбцы матрицы)
\\~\\
\textbf{(?)} Ещё про замечание про норму матрицы $L \times K$; $\|\cdot\|_{1, 2}$ --- это чисто сумма квадратов, делённая на $K$, верно?
\\

\noindent Сначала про главные отличия, а потом уже про детали.
\begin{enumerate}
\item В SVD столбцы и строки равноправны, в PCA --- нет. Подробности --- в деталях;
\item В АГК имеется предположение о том, что признаки центрированы (АГК по ковариационной матрице), также возможна их нормировка (АГК по корреляционной матрице)
\end{enumerate}


Если для сингулярного разложения матрицы $\bm Y$ слова 
<<\textit{ортонормированный} вектор $U_i$>>
вполне ясны и не требуют разъяснений, ибо рассматривается стандартная эвклидова норма $\|\bm a\|^2_1 = a_1^2 + \ldots + a_n^2$, то в случае анализа главных компонент матрица данных $\bm X = \bm Y^\mathrm{T}$ имеет определённую структуру, где строки есть индивиды, а столбцы --- признаки. В статистике разумно назначать каждому индивиду вес $\frac{1}{K}$, где $K$ --- количество индивидов, а признакам --- единицу. Поэтому норма для пространства строк-признаков матрицы $\bm Y$ выбирается как корень из среднего арифметического квадратов, а норма для пространства столбцов-индивидов $\bm Y$ --- как корень из суммы квадратов. 

Пусть у нас есть разложение $\bm Y = \sum_{i = 1}^d \sqrt{\lambda_i} U_i V_i^\mathrm{T}$. Так как $U_i \in \mathbb R^{L \times 1}$ есть ортонормированный базис пространства \textbf{столбцов} матрицы $\bm Y$, то норма в случае PCA не меняется, и следовательно в <<новом>> разложении $\bm Y = \sum_{i = 1}^d \sqrt{\widetilde \lambda_i} \widetilde U_i \widetilde V_i^\mathrm{T}$ получается, что $\widetilde U_i = U_i$. Для строк получается $\widetilde V_i = \sqrt{K}{V_i}$, а для собственных значений $\widetilde \lambda_i = \frac{\lambda_i}{K}$.

\subsection{Почему главные компоненты так называются, в каком смысле они главные}

Пусть у нас есть векторы $Y_1,\,\ldots,\, Y_K \in \mathbb{R}^L$. Хотим найти вектор $P$ с единичной нормой такой, который наилучшим образом описывает указанные элементы $\mathbb R^L$. Как известно,  длина проекции некоторого вектора $X$ на вектор $P$ равна $|\langle X, P \rangle|$; чем больше длина проекции, тем <<ближе>> вектор $P$ к вектору $X$. 

Можем сформулировать оптимизационную задачу, которая найдёт такую точку в $\mathbb R^L$, которая будет самой <<близкой>> к $Y_1,\,\ldots,\, Y_K$:
$$
P_1 = \argmax_{\widetilde P}{\sum_{i = 1}^{L}{\langle Y_i, \widetilde{P} \rangle ^2}}
$$
Данную компоненту будем называть \textbf{главной}.

Для того, чтобы найти следующий близкий к $Y_1,\,\ldots,\, Y_K$ вектор, рассматриваем ту же задачу, только уже в ортогональном для $P_1$ подпространстве, и так далее, пока не исчерпаем размерность.
Оказывается, что эти максимумы достигаются на векторах $U_i$, при этом максимумы суммы квадратов длин соответствующих проекций достигаются на $\lambda_i$. Так что компоненты главные, так как они наилучшим образом описывают совокупность старых признаков.

\textit{Заметим также, что длины соответствующих проекций можно найти в $Z_i = \bm X U_i = \sqrt{\lambda_i} V_i$.}

\subsection{Оптимальность сингулярного разложения в смысле аппроксимации матрицей ранга $r$}

Сразу приведём предложение.

\begin{proposition}[Аппроксимация матрицы матрицей меньшего ранга]
~\\
Обозначим за $M_r \subset \mathbb{R}^{L\times K}$ пространство матриц ранга, меньшего или равного $r$. Тогда для произвольной матрицы $\bm Y \in \mathbb{R}^{L\times K}$ ранга $d$ выполняется
$$\min_{\widetilde{\bm Y} \in M_r}{\|\bm Y -\widetilde{\bm Y} \|^2_F} = \sum_{i = r+1}^{d} \lambda_i,$$ причём минимум достигается на матрице $\sum_{i = 1}^d \sqrt{\lambda_i} U_i V_i^\mathrm{T}$.
\end{proposition}

\subsection{Оптимальность сингулярного разложения в смысле аппроксимации подпространством размерности $r$}

Сразу приведём предложение.

\begin{proposition}[Аппроксимация подпространством]
~\\
Пусть $\mathcal{L} \subset \mathbb{R}^L$ --- подпространство размерности $\leqslant r$. Тогда 
$$
\min_{\mathcal L_r} \left(\sum_{i = 1}^K\mathrm{dist}^2(Y_i, \mathcal L_r)\right) = \sum_{i = r + 1}^{d} \lambda_i
$$
и достигается на $\mathcal L_r^{(0)} = \mathrm{span}(U_1, \ldots, U_r)$.
\end{proposition}

\subsection{Оптимальность в АГК в статистической терминологии (через дисперсии)}

\textbf{(?)} На странице 21: получается, что норма $\|\cdot \|_{1,2}$ одинакова у $\bm X$ и $\bm X^\mathrm{T}$. Меня немного смущает то, на что мы делим в этом случае: у нас же столбцы и строки неравноправны, то есть в случае $\bm X^\mathrm{T}$ мы должны делить сумму квадратов на $K$,  a в случае $\bm X$ на $L$. (исправлено)
\\~\\
\textbf{(?)} На стр. 22: знаем, что квадрат нормы $Z_i$ есть его $s^2$. Почему? Это потому, что $Z_i$ является линейной комбинацией \textit{центрированных столбцов} матрицы $\bm X$? 
\\~\\
\textbf{(?)} На стр. 22: не понял с нормой; $\| (\sqrt{\lambda_i} U_i V_i^\mathrm{T})^\mathrm{T} \|^2_{1,2}?$ (исправлено)
\\~\\

Можно перейти на выборочный язык, то есть $n = K, p = L$, но пока останемся в старом варианте.

Применяя АГК к матрице $\bm Y$, получаем новые признаки, которые ранжированы по убыванию своей значимости в контексте объяснённых различий между индивидами. Покажем, что это имеет прямое отношение к дисперсиям признаков.

По умолчанию, столбцы $X_i$ центрированы, поэтому по определению $$\|X_i\|^2_2 = \frac{1}{K} \sum_{i = 1}^K ((X_i)_j)^2 = s^2(X_i).$$ 
В то же время, когда делаем АГК, ввиду ортонормированности $V_i$ по норме $\|\cdot\|_2$ а также ввиду того, что $Z_i$ есть линейная комбинация центрированных столбцов $X_i$
$$
\|Z_i\|^2_2 =s^2(Z_i) = \|\sqrt{\lambda_i} V_i\|^2_2 = \lambda_i 
$$
Поэтому, так как $\bm X^\mathrm{T} \in \mathbb{R}^{L \times K}$, её ранг равен $d$, имеет место цепочка равенств
$$
\| \bm X^\mathrm{T} \|^2_{1, 2} \quad \begin{matrix} =\boxed{\sum_{i = 1}^{L} s^2(X_i)} \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \\ \\  =\sum_{i = 1}^{d} \| (\sqrt{\lambda_i} U_i V_i^\mathrm{T}) \|^2_{1,2} = \sum_{i = 1}^d \lambda_i = \boxed{\sum_{i = 1}^ d s^2(Z_i)},  \end{matrix}
$$
то есть сумма дисперсий (\textit{total variance}) не меняется при переходе к новым признакам. 

Таким образом, можем первую главную компоненту интерпретировать как такой признак, который несёт в себе наибольшую часть информации о предыдущих признаках в совокупности. И так далее для второй, третьей, и т.\,д. Другими словами, главные компоненты являются ортогональными между собой линейными комбинациями исходных признаков, которые обладают свойством оптимальности.

\subsection{Оптимизация в АГК в терминах ковариационных матриц}
\textbf{(?)} $\| \cdot \|^2_{1,2}?$ (исправлено)
\\~\\


Пусть $\bm Y = X^\mathrm{T}$ --- транспонированная матрица данных. 

\begin{proposition}[Об эквивалентности двух задач оптимизации]
~\\
Рассмотрим две оптимизационные задачи:
\begin{equation}
\label{eq:sim}
\min_{\widetilde{\bm Y} \in M_r}{\|\bm Y -\widetilde{\bm Y} \|^2},
\end{equation}
\begin{equation}
\label{eq:cov}
\min_{\widetilde{\bm Y} \in M_r}{\|\bm Y \bm Y^\mathrm{T} -\widetilde{\bm Y} \widetilde{\bm Y}^\mathrm{T}  \|^2}.
\end{equation}
Задачи \ref{eq:sim} и \ref{eq:cov} эквивалентны. Их решение строится как сумма первых собственных троек, которые соответствуют первым $r$ главным компонентам.
\end{proposition}

Теперь, если матрица центрирована, то задача \ref{eq:cov} будет эквивалентна задаче нахождения <<ближайшей>> по оговорённой норме ковариационной матрицы меньшего ранга $r$.

\subsection{В двух статистических пакетах получились разные главные компоненты. Отчего так могло получиться?}

Это может произойти по нескольким причинам:

\begin{enumerate}
\item Уже говорили про единственность разложения: компоненты $U_i,\, V_i$ могут отличаться знаком, а также то, что некоторые $\lambda_i$ могут совпадать;
\item Может быть такое, например, что вектор главных компонент пакетом отображается как $V_i$, а не $\sqrt{\lambda_i} V_i$;
\item Для нахождения собственных чисел и собственных векторов применяются разные численные методы, что влечёт различия в ошибке;
\item Процедура может стандартизировать данные перед нахождением компонент, а это изменит решение если в другом случае данные не приведены к такому же виду.
\end{enumerate}

\subsection{Смысл первой ГК, если все ковариации (корреляции) исходных признаков положительны}

На этот вопрос отвечает

\begin{theorem}[Перрона--Фробениуса]
Пусть матрица $\bm A$ --- симметричная, неотрицательно определённая с положительными элементами. Тогда все компоненты её первого собственного вектора будут одного знака.
\end{theorem}

В нашем конкретном случае теорема утверждает, что если выборочные корреляции/ковариации положительны, то все координаты $U_1$ будут положительными. Так как вектор $U_1$ состоит из коэффициентов линейной комбинации старых признаков (с одинаковыми знаками), то первую главную компоненту можем интерпретировать как общий уровень какой-то характеристики.

\subsection{АГК по корреляционной и по ковариационной матрице. Когда что использовать}

Если \textit{только} центрируем наши признаки, то говорим, что АГК делаем \textbf{по ковариационной матрице}, так как 
$$
\frac{1}{n} \bm X^\mathrm{T}_{(c)} \bm X_{(c)}
$$
есть выборочная ковариационная матрица,
если же \textit{к тому же} стандартизируем, то получается выборочная корреляционная матрица
$$
\frac{1}{n} \bm X^\mathrm{T}_{(s)} \bm X_{(s)}
$$
и мы говорим, что делаем АГК \textbf{по корреляционной матрице}.

\paragraph{Когда что использовать?} Если достоверно известно, что разные признаки имеют разную шкалу, но при этом признаки рассматриваются как равноправные между собой, то их нужно стандартизировать. Если у признаков одинаковые шкалы, то это необязательно.

\subsection{Способы выбора числа главных компонент}
\textbf{(?)} Про правило сломанной кости, если можно, повторить. Там случайно не сумма должна стоять сверху? (там надо исправлять)
\\

Есть несколько вариантов:

\begin{enumerate}
\item Берётся столько компонент, сколько в совокупности даёт информативность не менее определённого числа;
\item \textbf{Правило Кайзера}: берутся те компоненты, информативность которых больше средней;
\item \textbf{Правило сломанной трости}: (?, не принципиально)
\item \textbf{Правило каменистой осыпи}: строим график собственных чисел по убыванию; берём компоненты до момента, когда собственные числа начинают мало отличаться друг от друга.
\end{enumerate}


\subsection{Почему доля собственного числа по отношению к сумме собственных чисел называется объясненной долей общей дисперсии?}

Для центрированных признаков дисперсия выглядит так:
$$\|X_i\|^2_2 = \frac{1}{K} \sum_{i = 1}^K ((X_i)_j)^2 = s^2(X_i).$$ 
В то же время, когда делаем АГК, ввиду ортонормированности $V_i$ по норме $\|\cdot\|_2$ а также ввиду того, что $Z_i$ есть линейная комбинация центрированных столбцов $X_i$
$$
\|Z_i\|^2_2 =s^2(Z_i) = \|\sqrt{\lambda_i} V_i\|^2_2 = \lambda_i .
$$
Следующее равенство показывает, что суммы дисперсий и сумма собственных чисел это одно и то же:
$$
\| \bm X^\mathrm{T} \|^2_{1, 2} \quad \begin{matrix} =\boxed{\sum_{i = 1}^{L} s^2(X_i)} \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \\ \\  =\sum_{i = 1}^{d} \| (\sqrt{\lambda_i} U_i V_i^\mathrm{T})^\mathrm{T} \|^2_{1,2} =\boxed{\sum_{i = 1}^d \lambda_i} = \boxed{\sum_{i = 1}^ d s^2(Z_i)},  \end{matrix}
$$
Поэтому для каждого нового признака $Z_i$ выражение
$$
\frac{\lambda_i}{\sum_{i = 1}^d \lambda_i}
$$
будет эквивалентно
$$
\frac{s^2(Z_i)}{\sum_{i = 1}^d s^2(Z_i)} = \frac{s^2(Z_i)}{\sum_{i = 1}^d s^2(X_i)},
$$
что и требовалось показать.

\subsection{На основе каких элементов сингулярного разложения интерпретируются главные компоненты как линейные комбинации исходных признаков? Привести формулу и пример}

Формула, которая это объясняет:
$$
\boxed{Z_i = \bm X U_i}
$$
Напомним, что $U_i$ есть собственные векторы матрицы $\bm X^\mathrm{T} \bm X$.

\subsection{АГК с точки зрения построения базиса в пространстве индивидов и в пространстве признаков. Координаты в новых базисах}

Итак, по построению 
\begin{itemize}
\item $\{U_i\}_{i = 1}^{d} \in \mathbb{R}^L$ --- ортонормированный базис в пространстве индивидов/строчек матрицы $\bm X$;
\item $\{V_i\}_{i = 1}^{d} \in \mathbb{R}^K$ --- ортонормированный базис в пространстве признаков/столбцов матрицы $\bm X$.
\end{itemize}
Основная формула для АГК, которая нам должна помочь:
$$
\bm Y = \bm X^\mathrm{T} = \sum_{i = 1}^{d} \sqrt{\lambda_i}U_i V_i^\mathrm{T} = \sum_{i = 1}^{d} F_i V_i^\mathrm{T} = \sum_{i = 1}^{d} U_i Z_i^\mathrm{T},
$$
где $F_i = \sqrt{\lambda_i} U_i$ --- вектор $i$-ых факторных компонент, $Z_i = \sqrt{\lambda_i} V_i$ --- вектор главных компонент.

\paragraph{Базис в пространстве признаков}

Если составить матрицу $\bm F$ из столбцов $F_i$, то элементы этой матрицы будут координатами \textit{старых признаков} в новом базисе признаков.
$$
f_{ij} = \langle X_i, V_j \rangle _2 = \begin{cases} \qquad \mathrm{Cov}(X_i, V_j),\,\qquad \textit{если АГК по ковариационной матрице} \\  \rho(X_i, V_j) = \rho(X_i, Z_j),\, \textit{если по корреляционной матрице} \end{cases}
$$
$f_{ij}$ --- $j$-ая координата $i$-го признака в новом базисе признаков.
Таким образом, чтобы найти в матрице $\bm F$ координаты первого старого признака, мы смотрим на первую строчку.

\paragraph{Базис в пространстве индивидов}

Если составить матрицу $\bm Z$ из столбцов $Z_i$, то элементы этой матрицы будут координатами индивидов в новом базисе признаков.

$$
z_{ij} = \langle \bm x_i, U_j \rangle _1
$$
$z_{ij}$ --- $j$-ая координата $i$-го индивида в новом базисе признаков.

\subsection{Как вычислить значения главных компонент для индивида, которого не было в исходной выборке. А как вычислить значения факторных значений?}
\textbf{(?)} Что такое значения факторных значений?

\paragraph{Значения главных компонент для новых индивидов}
Здесь используем то, что написано в прошлом вопросе. Было:
$$
z_{ij} = \langle \bm x_i, U_j \rangle _1.
$$
$z_{ij}$ --- $j$-ая координата $i$-го индивида в новом базисе признаков. Поэтому точно так же можно взять нового индивида $\tilde{\bm{x}}$ и разложить его по базису ${U_i}$: 
$$
\sum_{j = 1}^d \langle \widetilde{\bm{x}}, U_j \rangle _1 U_j.
$$

\paragraph{Значения факторных значений}

(?) мне ответили, но я не понял.

\subsection{В каком случае координаты в ортонормированном базисе можно назвать корреляциями?}

Координаты в ортонормированном базисе можно назвать корреляциями, если координаты есть некоторые скалярные произведения центрированных и стандартизированных векторов в пространстве с нормой $\|\cdot\|_2$. Можно привести пример с матрицей $\bm F$, которая состоит из скалярных произведений вида 
$$
\langle X_i, V_j \rangle_2.
$$

\subsection{Чему равны суммы квадратов по строкам и по столбцам в матрице, составленной из собственных векторов в АГК?}

Спрашивается про собственные векторы $U_i$, которые объединили в матрицу $\bm U$. По построению столбцы ортонормированы, поэтому сумма квадратов тут будет единица. 
Теперь по строчкам: сумма квадратов элементов по строке будет равна 
$$
\sum_{j = 1}^d u_{ij}^2 = \sum_{j = 1}^d  f_{ij}^2/\lambda_i = \frac{1}{\lambda_i} \sum_{j = 1}^d  f_{ij}^2 = \frac{\|X_i\|^2_2}{\lambda_i} = \frac{s^2(X_i)}{\lambda_i}.
$$
(?)

\subsection{Чему равны суммы квадратов по строкам и по столбцам в матрице факторных нагрузок в АГК?}

По определению $F_i = \sqrt{\lambda_i} U_i$, а также
$$
f_{ij} = \langle X_i, V_j \rangle _2 = \begin{cases} \qquad \mathrm{Cov}(X_i, V_j),\,\qquad \textit{если АГК по ковариационной матрице} \\  \rho(X_i, V_j) = \rho(X_i, Z_j),\, \textit{если по корреляционной матрице} \end{cases}.
$$
Отсюда
$$
\sum_{i = 1}^p  f_{ij}^2 = \|F_j\|^2 = \lambda_j
$$
$$
\sum_{j = 1}^d  f_{ij}^2 = \langle X_i, V_j  \rangle_2^2 = \|X_i\|^2_2 = \begin{cases}  s^2(X_i),\,\qquad \textit{если АГК по ковариационной матрице} \\ \qquad\,\, 1,\,\qquad \textit{если по корреляционной матрице} \end{cases}.
$$

\subsection{Как интерпретировать скалярное произведение строк в матрице факторных нагрузок в АГК?}
\textbf{(?)} 


\subsection{Как нарисовать исходные орты в плоскости первых двух главных компонент?}

Берём матрицу факторных нагрузок $\bm F$ и её первые два столбца. Первый столбец --- $x$-координаты старого признака, второй --- $y$-координаты.

\subsection{Зачем и когда первые две координаты факторных нагрузок рисуются в единичном круге?}

\paragraph{Зачем}
Для того, чтобы понять, какую долю дисперсии старого признака объясняют первые два новых признака. Чем ближе к единичной окружности, тем лучше.

\paragraph{Когда}
Только когда АГК у нас по корреляционной матрице, иначе total variance не будет равен единице, а значит нет смысла смотреть что-то в единичном круге.

\subsection{Чему равна норма $i$-го вектора из главных компонент?}

$$
\|Z_i\|^2_2 =s^2(Z_i) = \|\sqrt{\lambda_i} V_i\|^2_2 = \lambda_i .
$$

\end{document}


